{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Building from Scratch, Certain Basics\n",
    "\n",
    "# Let's suppose that I have three input layers\n",
    "# The weights represent the 'influence' of the layers on the ouputs\n",
    "# A simple way to understand what the bias is: \n",
    "# It is somehow similar to the constant b of a linear function y = ax + b\n",
    "# It allows me to move the line up and down to fit the prediction with the data better.\n",
    "# Without b, the line always goes through the origin (0, 0) and I may get a poorer fit.\n",
    "# The easiest case!!!\n",
    "\n",
    "inputs = [1.2, 5.1, 2.1]\n",
    "weights = [3.1, 2.1, 8.7]\n",
    "bias = 3\n",
    "output = sum(inputs[i]*weights[i] for i in range(2)) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODING OF A LAYER\n",
    "# 4 inputs (in the layer) and 3 outputs\n",
    "# Trying to predict failure or not for servers (for example)\n",
    "# So various sensors : heat, humidity, ... (inputs) \n",
    "# Different weights that represent different influences of the inputs on each output. \n",
    "# Hence weights[j] of len 4 : influence of the 4 inputs on output j\n",
    "# Each neuron from the ouptut will have its own separate bias\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output=[inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "        inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "        inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "\n",
    "\n",
    "# Inputs are often outputs from other neurons. Imagine that we want to change the output values, then we \n",
    "# would have to change the weights and biases. \n",
    "# The whole point of Deep Learning : how to tune best these values to do what is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move to a better code\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# First version\n",
    "layer_output_1 = []\n",
    "for j in range(len(weights)):\n",
    "    influence = sum(inputs[i]*weights[j][i] for i in range(len(inputs)))\n",
    "    layer_output_1.append(influence + biases[j])\n",
    "\n",
    "#Second version \n",
    "layer_ouput_2 = []\n",
    "for neuron_weight, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for input, weight in zip(inputs, neuron_weight):\n",
    "        neuron_output += input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_ouput_2.append(neuron_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Numpy, a list is a 1-D array\n",
    "# Example : lol=[[1,1,1,1], [1,2,3,4]] shape: (2,4) and type : 2D array, matrix \n",
    "# lolol = [ [[1,1,1,1],[2,3,5,1]], [[1,1,2,2],[2,3,4,5]], [[2,2,2,2],[9,4,5,6]] ]  shape : (3,2,4)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs1 = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "bias1 = 2\n",
    "output1 = np.dot(inputs1, weights1) + bias1 # dot : produit scalaire \n",
    "\n",
    "inputs2 = [1, 2, 3, 2.5]\n",
    "weights2 = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases2 = [2, 3, 0.5]\n",
    "output2 = np.dot(weights2, inputs2) + biases2\n",
    "\n",
    "# Inputs are features from a single sample (describe the current status of an object of interest)\n",
    "# I want a batch of these samples\n",
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], \n",
    "           [0.5, -0.91, 0.26, -0.5], \n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# If I multiply directly by taking the dot, problem of dimensions. I need to transpose.\n",
    "# What I did previously works only if I have one sample. \n",
    "# [2.8  -1.79  1.885]                                [ 4.8  1.21  2.385]\n",
    "# [6.9  -4.81  -0.3]        +   [2.O  3.0  0.5]   =  [8.9  -1.81  0.2]\n",
    "# [-0.59 -1.989  -0.474]                                .....\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "# Add layer\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "weights_layer = [[0.1, -0.14, 0.5], \n",
    "           [-0.5, 0.12, -0.33], \n",
    "           [-0.44, -0.73, -0.13]]\n",
    "biases_layer = [-1, 2, -0.5]\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights_layer).T) + biases_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "# 0.10 because we want the weights to be small\n",
    "# format of a matrix so we don't have to transpose. If 4 inputs and 3 layers :\n",
    "# self.weights returns 4*3 matrix where i,j : weight of input i on neuron j  \n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = (0.10) * np.random.randn(n_inputs, n_neurons)                                                       \n",
    "        self.biases = np.zeros((1, n_neurons)) # The format is always in 'shape': 1 array\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Thanks to the definition of the class, Layer_Dense(4,5) create a dense layer with :\n",
    "# 4 input features and 5 neurons. This creates a weight matrix of shape (4,5). \n",
    "# And a bias vector of shape(1,5) \n",
    "\n",
    "layer1 = Layer_Dense(4,5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "layer1.forward(X)\n",
    "#print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "#print(layer2.output)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spiral dataset\n",
    "#https://cs231n.github.io/neural-networks-case-study/ inspired by this generated data\n",
    "\n",
    "def create_data(points, classes):\n",
    "    X = np.zeros((points*classes,2)) # data matrix (each row = single example)\n",
    "    y = np.zeros(points*classes, dtype='uint8') # class labels\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0,1,points) # radius\n",
    "        t = np.linspace(class_number*4,(class_number+1)*4, points) + np.random.randn(points)*0.2 # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = create_data(100,3)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"rainbow\", s=20)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Synthetic Spiral Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Illustration of the spiral dataset] /Users/noam/Desktop/Code/Spiral-data-BNN.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = (0.10) * np.random.randn(n_inputs, n_neurons)                                                       \n",
    "        self.biases = np.zeros((1, n_neurons)) # The format is always in 'shape': 1 array\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# Activation function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "layer1 = Layer_Dense(2,5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "#print(layer1.output)\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to train the model (making the error : what is out -\n",
    "# what is expected as small as possible). \n",
    "# Infos sent back to the hidden layers who 'learn'\n",
    "# To do that, the outputs have to 'make sense' for us\n",
    "# Softmax activation function : transforms the ouput in probabilities\n",
    "# Keeps the sense of the ouput : negative values are smaller than positive thanks to the exp\n",
    "# We normalize so the outputs are now probabilities\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "# exp_values = [math.exp(x) for x in layer_outputs]\n",
    "exp_values = np.exp(layer_outputs)\n",
    "# normalized_value = np.sum(exp_values)\n",
    "normalized_output = exp_values / np.sum(exp_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like previously but with a batch this time to train the model\n",
    "# axis=1 somme par lignes, axis=0 somme par colonnes\n",
    "# keepdims allows transposition\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "                 [8.9, -1.81, 0.2],\n",
    "                 [1.41, 1.051, 0.026]]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "row_sum = np.sum(layer_outputs, axis=1, keepdims=True)\n",
    "output = exp_values / row_sum\n",
    "\n",
    "\n",
    "# Substraction of the max value method\n",
    "# exp grows significantly and error often associated with big values of exp\n",
    "# for every output, output - max(ouput) and then exp, end probabilities are the same\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = (0.10) * np.random.randn(n_inputs, n_neurons)                                                       \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Activation function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "# Ici, l'input sera un output puisque on travaille avec des layers\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "X,y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "activation2.output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some measure of how wrong the model is : loss function\n",
    "# Here, softmax classifier with categorical cross-entropy\n",
    "# One batch\n",
    "\n",
    "import math\n",
    "softmax_output = [0.7, 0.1, 0.2] # output out of my activation function after layer\n",
    "target_output = [1, 0, 0]\n",
    "loss = -sum(math.log(softmax_output[i])*target_output[i] for i in range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes : 0-dog, 1-cat, 2-human\n",
    "# Imagine that the 3 images (3-sample) we test are dog, cat, cat : class_targets = [0, 1, 1]\n",
    "import numpy as np\n",
    "\n",
    "y_pred_clipped = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "y_true = [0, 1, 1]\n",
    "samples = 3\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = (0.10) * np.random.randn(n_inputs, n_neurons)                                                       \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Activation function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "# Ici, l'input sera un output puisque on travaille avec des layers\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "# y_pred is what goes out, y_pred is what we whould observe\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # y_pred_clipped = [[0.7, 0.1, 0.2],                [0,    [0,         [0.7,\n",
    "        #                   [0.1, 0.5, 0.4],   samples = 3   1,  ,  1,   =      0.5,\n",
    "        #                   [0.02, 0.9, 0.08]]               2]     1]          0.9]\n",
    "\n",
    "        elif len(y_true.shape) ==2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "X,y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Neuron Network\n",
    "# 2 inputs, 1 output : simple classifier to distinguish 2 groups False and True\n",
    "# If False and True = True.  True and False = False. T and T = T.   F and F = F.\n",
    "# We translate this situation with 0 for False and 1 for True\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "weights = np.random.rand(3)\n",
    "bias = 1\n",
    "learning_rate = 1\n",
    "\n",
    "def Heaviside_activation_function(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Network(input1, input2, output):\n",
    "    network_pre_output = input1*weights[0] + input2*weights[1] + bias*weights[2]\n",
    "    network_output = Heaviside_activation_function(network_pre_output)\n",
    "    error = output - network_output\n",
    "    weights[0] += error*input1*learning_rate\n",
    "    weights[1] += error*input2*learning_rate\n",
    "    weights[2] += error*bias*learning_rate\n",
    "\n",
    "# Learning phase : tout le but de cette phase va etre d'ajuster les 3 coeffs de weights \n",
    "\n",
    "for _ in range(60):\n",
    "    Network(0, 0, 0)\n",
    "    Network(1, 1, 1)\n",
    "    Network(0, 1, 1)\n",
    "    Network(1, 0, 1)\n",
    "\n",
    "# Testing function\n",
    "\n",
    "def testing_Network(input1, input2):\n",
    "    pre_output = input1*weights[0] + input2*weights[1] +bias*weights[2]\n",
    "    return Heaviside_activation_function(pre_output)\n",
    "\n",
    "print(\"Testing the trained network:\")\n",
    "print(f\"Input (0, 0): Output -> {testing_Network(0, 0)}\")  \n",
    "print(f\"Input (1, 0): Output -> {testing_Network(1, 0)}\")\n",
    "print(f\"Input (0, 1): Output -> {testing_Network(0, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
